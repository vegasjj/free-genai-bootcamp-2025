networks:
  megaservice_network:
    driver: bridge
services:
  #megaservice:
  #  build:
  #    context: .
  #    dockerfile: Dockerfile
  #  container_name: megaservice
  #  #depends_on:
  #  #  - redis-vector-db
  #  #  - tei-embedding-service
  #  #  - retriever
  #  #  - tei-reranking-service
  #  #  - vllm-service
  #  ports:
  #    - "8888:8888"
  #  ipc: host
  #  restart: always
  speecht5-service:
    image: ${REGISTRY:-opea}/speecht5:${TAG:-latest}
    container_name: speecht5-service
    ports:
      - ${SPEECHT5_PORT:-7055}:7055
    ipc: host
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7055/health"]
      interval: 10s
      timeout: 6s
      retries: 18
  #tts-speecht5:
  #  image: ${REGISTRY:-opea}/tts:${TAG:-latest}
  #  container_name: tts-speecht5-service
  #  ports:
  #    - ${TTS_PORT:-9088}:9088
  #  ipc: host
  #  environment:
  #    TTS_ENDPOINT: http://172.24.230.22:7055
  #    TTS_COMPONENT_NAME: ${TTS_COMPONENT_NAME:-OPEA_SPEECHT5_TTS}
  #  depends_on:
  #    speecht5-service:
  #      condition: service_healthy
  gptsovits-service:
    image: ${REGISTRY:-opea}/gpt-sovits:${TAG:-latest}
    container_name: gpt-sovits-service
    ports:
      - ${GPT_SOVITS_PORT:-9880}:9880
    ipc: host
    volumes:
      - ./audio:/audio
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9880/health"]
      interval: 10s
      timeout: 6s
      retries: 18
  tts-gptsovits:
    image: ${REGISTRY:-opea}/tts:${TAG:-latest}
    container_name: tts-gptsovits-service
    ports:
      - ${TTS_PORT:-9088}:9088
    ipc: host
    environment:
      TTS_ENDPOINT: http://10.0.10.244:9880
      TTS_COMPONENT_NAME: ${TTS_COMPONENT_NAME:-OPEA_GPTSOVITS_TTS}
    depends_on:
      gptsovits-service:
        condition: service_healthy
  #vllm-service:
  #  image: ${REGISTRY:-opea}/vllm:${TAG:-latest}
  #  container_name: vllm-service
  #  ports:
  #    - "9009:80"
  #  volumes:
  #    - "./data:/data"
  #  shm_size: 128g
  #  environment:
  #    no_proxy: ${no_proxy}
  #    http_proxy: ${http_proxy}
  #    https_proxy: ${https_proxy}
  #    HF_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
  #    LLM_MODEL_ID: ${LLM_MODEL_ID}
  #    VLLM_TORCH_PROFILER_DIR: "/mnt"
  #  deploy:
  #    resources:
  #      reservations:
  #        devices:
  #          - driver: nvidia
  #            count: 1
  #            capabilities: [gpu]
  #  healthcheck:
  #    test: ["CMD-SHELL", "curl -f http://$host_ip:9009/health || exit 1"]
  #    interval: 10s
  #    timeout: 10s
  #    retries: 100
  #  command: --model meta-llama/Llama-3.2-1B --host 0.0.0.0 --port 80